# Спринт 14. Потоковая обработка данных

## Описание

В учебном проекте, если условия рекламных кампаний и местоположение пользователя совпадали по заданным признакам, то ваш сервис отправлял сообщение в сервис push-уведомлений, а уже этот сервис push-уведомлений — уведомления пользователям.
Нужно создать приложение, которое будет сужать круг пользователей ещё больше и доставлять уведомления об акциях с ограниченным сроком действия.

## Цель работы

Создать сервис push-уведомлений для пользователей приложения, направленный на получение данных об акциях ресторана. 

Сервис должен будет:

- читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени;
- получать список подписчиков из базы данных Postgres;
- джойнить данные из Kafka с данными из БД;
- сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka;
- отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане;
- вставлять записи в Postgres, чтобы впоследствии получить фидбэк от пользователя.

 Сервис push-уведомлений должен будет читать сообщения из Kafka и формировать готовые уведомления.

## Этапы работы

1. Проверить работу потока.
2. Прочитать данные об акциях из Kafka.
3. Прочитать данные о подписчиках из Postgres.
4. Преобразование JSON в датафейм.
5. Провести JOIN потоковых и статичных данных.
6. Отправить результаты JOIN в Postgres для аналитики фидбэка.
7. Отправить данные, сериализованные в формат JSON, в Kafka для push-уведомлений.
8. Обеспечить персистентность датафрейма.

## Описание рабочих файлов

Папка src/project. 

- project.py - файл с итоговым проектом
- configs.py - файл с параметрами подключений и логины
- kafka_functions.py - функции работы с kafka
- pg_functions.py - функции работы с postgres